{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check paper for information<br>\n",
    "paper : https://arxiv.org/pdf/1806.06397"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review , instanse normalization ?\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from casnet import CasNet\n",
    "from patchGAN import PatchGAN\n",
    "from loss_functions import CGANGeneratorLoss,CGANDiscreminatorLoss,PerceptualLoss,StyleTransferLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters<br>\n",
    "note : only some of them , there are other hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "    \"gpu\":False,\n",
    "    \"g_lr\":0.0002,\n",
    "    \"d_lr\":0.0002,\n",
    "    \"batch_size\":1,\n",
    "    \"epochs\":1, # 200 in paper\n",
    "    \"g_iter\":3, # generator iteration per epoch\n",
    "    \"lambda_perceptual\":20,\n",
    "    \"lambda_style\": 0.0001,\n",
    "    \"lambda_content\": 0.0001,\n",
    "    \"lambda_perceptual_layer\":[1,1],\n",
    "    \"lambda_content_layer\":[1,2,3,4,5],\n",
    "    \"lambda_style_layer\":[1,0,0,0,1],\n",
    "    \"save_model_every_n_epoch\":50,\n",
    "    \"print_every_n_batch\":1,\n",
    "    \"percent_of_data_used\":1, # 1 means all\n",
    "    \"train_test_split\":1 # between 0 and 1 , provide train size\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(root_dir,\"dataset\")\n",
    "domain_A_data_dir = os.path.join(dataset_dir,\"A\")\n",
    "domain_B_data_dir = os.path.join(dataset_dir,\"B\")\n",
    "logs_dir = os.path.join(root_dir,\"logs\")\n",
    "\n",
    "current_run_log_fname = str(datetime.datetime.now()).replace(\":\",\"-\")[:-7]\n",
    "current_logs_dir = os.path.join(logs_dir,current_run_log_fname)\n",
    "os.makedirs(current_logs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,transform=None):\n",
    "        self.file_names = os.listdir(domain_A_data_dir)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imageA = read_image(os.path.join(domain_A_data_dir,self.file_names[idx]))\n",
    "        imageB = read_image(os.path.join(domain_B_data_dir,self.file_names[idx]))\n",
    "        if self.transform:\n",
    "            imageA = self.transform(imageA)\n",
    "            imageB = self.transform(imageB)\n",
    "        return imageA,imageB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "def get_transform():\n",
    "    transforms = []\n",
    "    transforms.append(T.Grayscale())\n",
    "    transforms.append(T.Resize((1024,1024)))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(transform=get_transform())\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "indices = indices[:int(len(indices)*hp[\"percent_of_data_used\"])] # number of data used\n",
    "split_idx = int(hp[\"train_test_split\"]*len(indices)) \n",
    "dataset_train = torch.utils.data.Subset(dataset, indices[:split_idx])\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[split_idx:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=hp[\"batch_size\"],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if(not hp[\"gpu\"]):\n",
    "    device = \"cpu\"\n",
    "    \n",
    "else:\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator =  CasNet(num_ublocks=6).to(device)\n",
    "discriminator = PatchGAN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(weights='DEFAULT').features.to(device)\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=hp[\"d_lr\"])\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=hp[\"g_lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_loss_generator_fn=CGANGeneratorLoss()\n",
    "cgan_loss_discriminator_fn=CGANDiscreminatorLoss()\n",
    "perceptual_loss_fn = PerceptualLoss()\n",
    "style_transfer_loss_fn=StyleTransferLoss(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1]\n"
     ]
    }
   ],
   "source": [
    "losses = {\"g_loss\":[],\"d_loss\":[],\"g_cgan\":[],\"g_perceptual\":[],\"g_style\":[],\"g_content\":[]}\n",
    "size = len(data_loader.dataset)\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(hp[\"epochs\"]):\n",
    "    print(f\"epoch [{epoch+1}/{hp[\"epochs\"]}]\")\n",
    "    for batch, (A, B) in enumerate(data_loader):\n",
    "        A, B = A.to(device), B.to(device)\n",
    "        \n",
    "        # generator\n",
    "        for g_iter in range(hp[\"g_iter\"]):\n",
    "            generated = generator(A)\n",
    "            patch_y,fmap_y = discriminator(generated)\n",
    "            patch_x,fmap_x = discriminator(B)\n",
    "\n",
    "            # loss\n",
    "            g_cgan_loss = cgan_loss_generator_fn(patch_y)\n",
    "            perceptual_loss = perceptual_loss_fn(fmap_x,fmap_y,hp[\"lambda_perceptual\"],hp[\"lambda_perceptual_layer\"])\n",
    "            content_loss,style_loss = style_transfer_loss_fn(B,generated,vgg,hp[\"lambda_content\"]\\\n",
    "                ,hp[\"lambda_style\"],hp[\"lambda_content_layer\"],hp[\"lambda_style_layer\"])\n",
    "            g_loss = g_cgan_loss+perceptual_loss+content_loss+style_loss\n",
    "            \n",
    "            # backpropagation\n",
    "            g_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "            generator_optimizer.zero_grad()\n",
    "        \n",
    "        # discriminator review\n",
    "        generated = generator(A)\n",
    "        patch_y,fmap_y = discriminator(generated)\n",
    "        patch_x,fmap_x = discriminator(B)\n",
    "        d_loss = cgan_loss_discriminator_fn(B, generated, patch_x, patch_y)\n",
    "\n",
    "        d_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        losses[\"g_loss\"].append(g_loss)\n",
    "        losses[\"d_loss\"].append(d_loss)\n",
    "        losses[\"g_cgan\"].append(g_cgan_loss)\n",
    "        losses[\"g_perceptual\"].append(perceptual_loss)\n",
    "        losses[\"g_style\"].append(style_loss)\n",
    "        losses[\"g_content\"].append(content_loss)\n",
    "\n",
    "        if batch % hp[\"print_every_n_batch\"] == 0:\n",
    "            current = (batch + 1) * len(A)\n",
    "            print(f\"----g_loss: {g_loss:>7f} & d_loss: {d_loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    if(epoch%hp[\"save_model_every_n_epoch\"]==0):\n",
    "        torch.save(generator.state_dict(), os.path.join(current_logs_dir,\"generator-\"+str(epoch+1)+\".pth\"))\n",
    "        torch.save(discriminator.state_dict(), os.path.join(current_logs_dir,\"discriminator-\"+str(epoch+1)+\".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), os.path.join(current_logs_dir,\"final_generator.pth\"))\n",
    "torch.save(discriminator.state_dict(), os.path.join(current_logs_dir,\"final_discriminator.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in losses.items():\n",
    "    plt.plot(l)\n",
    "plt.legend(list(losses.keys()))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrcnn-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
